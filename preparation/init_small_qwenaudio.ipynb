{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "import librosa\n",
    "from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:20<00:00,  4.06s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n",
    "\n",
    "audio_tower = model.audio_tower\n",
    "multi_modal_projector = model.multi_modal_projector\n",
    "language_model = model.language_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Tower: 0.64 billion parameters\n",
      "Multi-Modal Projector: 0.01 billion parameters\n",
      "Language Model: 7.75 billion parameters\n",
      "Total Model: 8.40 billion parameters\n",
      "Qwen2AudioConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-Audio-7B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2AudioForConditionalGeneration\"\n",
      "  ],\n",
      "  \"audio_config\": {\n",
      "    \"model_type\": \"qwen2_audio_encoder\"\n",
      "  },\n",
      "  \"audio_token_index\": 151646,\n",
      "  \"ignore_index\": -100,\n",
      "  \"model_type\": \"qwen2_audio\",\n",
      "  \"text_config\": {\n",
      "    \"bos_token_id\": 151643,\n",
      "    \"eos_token_id\": 151645,\n",
      "    \"intermediate_size\": 11008,\n",
      "    \"max_position_embeddings\": 8192,\n",
      "    \"model_type\": \"qwen2\",\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"use_mrope\": false,\n",
      "    \"vocab_size\": 156032\n",
      "  },\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"vocab_size\": 156032\n",
      "}\n",
      "\n",
      "______________________\n",
      "Qwen2Config {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 156032\n",
      "}\n",
      "\n",
      "model.embed_tokens.weight torch.Size([156032, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.bias torch.Size([4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.v_proj.bias torch.Size([4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.norm.weight torch.Size([4096])\n",
      "lm_head.weight torch.Size([156032, 4096])\n"
     ]
    }
   ],
   "source": [
    "config = model.config\n",
    "\n",
    "audio_tower_config = audio_tower.config\n",
    "# multi_modal_projector_config = multi_modal_projector.config\n",
    "language_model_config = language_model.config\n",
    "\n",
    "# print how many billions of parameters in each model \n",
    "# Function to count parameters in billions\n",
    "def count_parameters_in_billions(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e9\n",
    "\n",
    "# Count and print parameters for each component\n",
    "print(f\"Audio Tower: {count_parameters_in_billions(audio_tower):.2f} billion parameters\")\n",
    "print(f\"Multi-Modal Projector: {count_parameters_in_billions(multi_modal_projector):.2f} billion parameters\")\n",
    "print(f\"Language Model: {count_parameters_in_billions(language_model):.2f} billion parameters\")\n",
    "print(f\"Total Model: {count_parameters_in_billions(model):.2f} billion parameters\")\n",
    "\n",
    " \n",
    "print(config)\n",
    "print(\"______________________\")\n",
    "print(language_model_config)\n",
    "# get the model weights, and print all layer names\n",
    "weights = language_model.state_dict()\n",
    "for key in weights:\n",
    "    print(key, weights[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def uniform_element_selection(wt, s_shape):\n",
    "    assert wt.dim() == len(s_shape), \"Tensors have different number of dimensions\"\n",
    "    ws = wt.clone()\n",
    "    for dim in range(wt.dim()):\n",
    "        assert wt.shape[dim] >= s_shape[dim], \"Teacher's dimension should not be smaller than student's dimension\"  # determine whether teacher is larger than student on this dimension\n",
    "        if wt.shape[dim] % s_shape[dim] == 0:\n",
    "            step = wt.shape[dim] // s_shape[dim]\n",
    "            indices = torch.arange(s_shape[dim]) * step\n",
    "        else:\n",
    "            indices = torch.round(torch.linspace(0, wt.shape[dim]-1, s_shape[dim])).long()\n",
    "        \n",
    "        #print(indices)\n",
    "        ws = torch.index_select(ws, dim, indices)\n",
    "    assert ws.shape == s_shape\n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.529933824\n",
      "model.embed_tokens.weight torch.Size([156032, 4096]) torch.Size([156032, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.q_proj.bias torch.Size([4096]) torch.Size([4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([8192, 4096])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Teacher's dimension should not be smaller than student's dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m     student_model\u001b[38;5;241m.\u001b[39mload_state_dict(student_weights)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m student_model\n\u001b[0;32m---> 37\u001b[0m small_model \u001b[38;5;241m=\u001b[39m \u001b[43mmake_small_qwen_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 29\u001b[0m, in \u001b[0;36mmake_small_qwen_audio\u001b[0;34m(teacher_model)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key, teacher_weights[key]\u001b[38;5;241m.\u001b[39mshape, student_weights[key]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m     s_shape \u001b[38;5;241m=\u001b[39m student_weights[key]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 29\u001b[0m     weight_selection[key] \u001b[38;5;241m=\u001b[39m \u001b[43muniform_element_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m student_weights\u001b[38;5;241m.\u001b[39mupdate(weight_selection)\n\u001b[1;32m     32\u001b[0m student_model\u001b[38;5;241m.\u001b[39mload_state_dict(student_weights)\n",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m, in \u001b[0;36muniform_element_selection\u001b[0;34m(wt, s_shape)\u001b[0m\n\u001b[1;32m      6\u001b[0m ws \u001b[38;5;241m=\u001b[39m wt\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(wt\u001b[38;5;241m.\u001b[39mdim()):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m wt\u001b[38;5;241m.\u001b[39mshape[dim] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m s_shape[dim], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeacher\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms dimension should not be smaller than student\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms dimension\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# determine whether teacher is larger than student on this dimension\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wt\u001b[38;5;241m.\u001b[39mshape[dim] \u001b[38;5;241m%\u001b[39m s_shape[dim] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m         step \u001b[38;5;241m=\u001b[39m wt\u001b[38;5;241m.\u001b[39mshape[dim] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m s_shape[dim]\n",
      "\u001b[0;31mAssertionError\u001b[0m: Teacher's dimension should not be smaller than student's dimension"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "#   \"hidden_size\": 4096,\n",
    "#   \"intermediate_size\": 11008,\n",
    "#   \"num_attention_heads\": 32,\n",
    "#   \"num_hidden_layers\": 32,\n",
    "#   \"num_key_value_heads\": 32,\n",
    "\n",
    "def make_small_qwen_audio(teacher_model):\n",
    "    student_config = copy.deepcopy(teacher_model.config)\n",
    "\n",
    "    student_config.hidden_size = 2048\n",
    "    student_config.num_hidden_layers = 2\n",
    "    student_config.intermediate_size = 2048\n",
    "\n",
    "    student_model = AutoModelForCausalLM.from_config(student_config, attn_implementation=config._attn_implementation)\n",
    "\n",
    "    print(student_model.num_parameters() / 1e9)\n",
    "\n",
    "    teacher_weights = teacher_model.state_dict()\n",
    "    student_weights = student_model.state_dict()\n",
    "    weight_selection = {}\n",
    "    for key in student_weights.keys():\n",
    "        # We don't perform weight selection on classification head by default. Remove this constraint if target dataset is the same as teacher's.\n",
    "        # if \"head\" in key:\n",
    "        #     continue\n",
    "        print(key, teacher_weights[key].shape, student_weights[key].shape)\n",
    "        s_shape = student_weights[key].shape\n",
    "        weight_selection[key] = uniform_element_selection(teacher_weights[key], s_shape)\n",
    "        \n",
    "    student_weights.update(weight_selection)\n",
    "    student_model.load_state_dict(student_weights)\n",
    "\n",
    "    return student_model\n",
    "\n",
    "\n",
    "small_model = make_small_qwen_audio(language_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 9])\n",
      "attention_mask torch.Size([1, 9])\n",
      "input_features torch.Size([1, 128, 3000])\n",
      "feature_attention_mask torch.Size([1, 3000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [8, 2048] cannot be broadcast to indexing result of shape [8, 4096]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(k, v\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 15\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m generated_ids[:, inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_ids)\n",
      "File \u001b[0;32m~/miniconda3/envs/qwenaudio/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwenaudio/lib/python3.10/site-packages/transformers/generation/utils.py:2050\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2042\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2043\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2044\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2045\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2046\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2047\u001b[0m     )\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2050\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2061\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2062\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2063\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2064\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2069\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2070\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/qwenaudio/lib/python3.10/site-packages/transformers/generation/utils.py:3000\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2997\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2999\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3000\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3003\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwenaudio/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwenaudio/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwenaudio/lib/python3.10/site-packages/transformers/models/qwen2_audio/modeling_qwen2_audio.py:1209\u001b[0m, in \u001b[0;36mQwen2AudioForConditionalGeneration.forward\u001b[0;34m(self, input_ids, input_features, attention_mask, feature_attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         selected_audio_feature \u001b[38;5;241m=\u001b[39m audio_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m   1207\u001b[0m         audio_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_modal_projector(selected_audio_feature)\n\u001b[0;32m-> 1209\u001b[0m         inputs_embeds, attention_mask, labels, position_ids, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_input_ids_with_audio_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m            \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_output_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model(\n\u001b[1;32m   1214\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1215\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1222\u001b[0m )\n\u001b[1;32m   1224\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/qwenaudio/lib/python3.10/site-packages/transformers/models/qwen2_audio/modeling_qwen2_audio.py:1072\u001b[0m, in \u001b[0;36mQwen2AudioForConditionalGeneration._merge_input_ids_with_audio_features\u001b[0;34m(self, audio_features, num_audio_tokens, inputs_embeds, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m   1066\u001b[0m final_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(\n\u001b[1;32m   1067\u001b[0m     (batch_size, max_token_num), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id, dtype\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39minputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1068\u001b[0m )\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# 4. Fill the embeddings based on the mask. If we have [\"hey\" \"<audio>\", \"how\", \"are\"]\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;66;03m# we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the audio features\u001b[39;00m\n\u001b[0;32m-> 1072\u001b[0m \u001b[43mfinal_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_to_overwrite\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m inputs_embeds[batch_indices, non_audio_indices]\n\u001b[1;32m   1073\u001b[0m final_attention_mask[batch_indices, text_to_overwrite] \u001b[38;5;241m=\u001b[39m attention_mask[batch_indices, non_audio_indices]\n\u001b[1;32m   1074\u001b[0m final_input_ids[batch_indices, text_to_overwrite] \u001b[38;5;241m=\u001b[39m input_ids[batch_indices, non_audio_indices]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [8, 2048] cannot be broadcast to indexing result of shape [8, 4096]"
     ]
    }
   ],
   "source": [
    "\n",
    "small_model.eval()\n",
    "model.language_model = small_model\n",
    "# model.language_model = language_model\n",
    "# test a bit\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n",
    "\n",
    "prompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\n",
    "audio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\n",
    "inputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n",
    "\n",
    "for k, v in inputs.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_length=256)\n",
    "generated_ids = generated_ids[:, inputs.input_ids.size(1):]\n",
    "print(generated_ids)\n",
    "response = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(\"______________________\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "def compare_models(model1, model2):\n",
    "\n",
    "    print(\"model1 training: \", model1.training, \" model2 training: \", model2.training)\n",
    "\n",
    "    # 先比较config\n",
    "    # Compare the configurations\n",
    "    config1 = model1.config.to_dict()\n",
    "    config2 = model2.config.to_dict()\n",
    "\n",
    "    print(config1)\n",
    "    print(\"______________________\")\n",
    "    print(config2)\n",
    "\n",
    "    for key in config1:\n",
    "        if key in config2:\n",
    "            if config1[key] != config2[key]:\n",
    "                print(f\"Different in {key}: Model1 - {config1[key]}, Model2 - {config2[key]}\")\n",
    "        else:\n",
    "            print(f\"Key {key} not found in Model2's config\")\n",
    "\n",
    "    for key in config2:\n",
    "        if key not in config1:\n",
    "            print(f\"Key {key} not found in Model1's config\")\n",
    "   \n",
    "\n",
    "    # Retrieve the state dictionaries\n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "\n",
    "    # For each parameter in the state dictionaries\n",
    "    for param1, param2 in zip(state_dict1.items(), state_dict2.items()):\n",
    "        # Unpack the parameter names and tensors\n",
    "        name1, tensor1 = param1\n",
    "        name2, tensor2 = param2\n",
    "\n",
    "        # Check the names are the same (they should be, if the models are of the same architecture)\n",
    "        if name1 == name2:\n",
    "            try:\n",
    "                # Calculate the difference between the two tensors\n",
    "                difference = torch.nn.functional.mse_loss(tensor1, tensor2)\n",
    "                \n",
    "                print(f\"Difference in {name1}: {difference}\")\n",
    "            except:\n",
    "                print(f\"Cannot compare {name1} and {name2}\")\n",
    "        else:\n",
    "            print(f\"Parameter names do not match: {name1}, {name2}\")\n",
    "\n",
    "\n",
    "\n",
    "compare_models(language_model, small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwenaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
