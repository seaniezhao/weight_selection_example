{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "import librosa\n",
    "from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration, AutoModelForCausalLM\n",
    "from transformers.models.qwen2_audio.modeling_qwen2_audio import Qwen2AudioMultiModalProjector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n",
    "\n",
    "audio_tower = model.audio_tower\n",
    "multi_modal_projector = model.multi_modal_projector\n",
    "language_model = model.language_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Tower: 0.64 billion parameters\n",
      "Multi-Modal Projector: 0.01 billion parameters\n",
      "Language Model: 7.75 billion parameters\n",
      "Total Model: 1.39 billion parameters\n",
      "Qwen2AudioConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-Audio-7B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2AudioForConditionalGeneration\"\n",
      "  ],\n",
      "  \"audio_config\": {\n",
      "    \"model_type\": \"qwen2_audio_encoder\"\n",
      "  },\n",
      "  \"audio_token_index\": 151646,\n",
      "  \"ignore_index\": -100,\n",
      "  \"model_type\": \"qwen2_audio\",\n",
      "  \"text_config\": {\n",
      "    \"bos_token_id\": 151643,\n",
      "    \"eos_token_id\": 151645,\n",
      "    \"hidden_size\": 2048,\n",
      "    \"intermediate_size\": 2048,\n",
      "    \"max_position_embeddings\": 8192,\n",
      "    \"model_type\": \"qwen2\",\n",
      "    \"num_hidden_layers\": 4,\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"use_mrope\": false,\n",
      "    \"vocab_size\": 156032\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"vocab_size\": 156032\n",
      "}\n",
      "\n",
      "2048\n",
      "______________________\n",
      "Qwen2AudioEncoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"d_model\": 1280,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 20,\n",
      "  \"encoder_ffn_dim\": 5120,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 32,\n",
      "  \"init_std\": 0.02,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"model_type\": \"qwen2_audio_encoder\",\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_mel_bins\": 128,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.45.0.dev0\"\n",
      "}\n",
      "\n",
      "______________________\n",
      "Qwen2Config {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 156032\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = model.config\n",
    "\n",
    "audio_tower_config = audio_tower.config\n",
    "# multi_modal_projector_config = multi_modal_projector.config\n",
    "language_model_config = language_model.config\n",
    "\n",
    "# print how many billions of parameters in each model \n",
    "# Function to count parameters in billions\n",
    "def count_parameters_in_billions(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e9\n",
    "\n",
    "# Count and print parameters for each component\n",
    "print(f\"Audio Tower: {count_parameters_in_billions(audio_tower):.2f} billion parameters\")\n",
    "print(f\"Multi-Modal Projector: {count_parameters_in_billions(multi_modal_projector):.2f} billion parameters\")\n",
    "print(f\"Language Model: {count_parameters_in_billions(language_model):.2f} billion parameters\")\n",
    "print(f\"Total Model: {count_parameters_in_billions(model):.2f} billion parameters\")\n",
    "\n",
    " \n",
    "print(config)\n",
    "print(config.text_config.hidden_size)\n",
    "print(\"______________________\")\n",
    "print(audio_tower_config)\n",
    "print(\"______________________\")\n",
    "print(language_model_config)\n",
    "\n",
    "# get the model weights, and print all layer names\n",
    "# weights = language_model.state_dict()\n",
    "# for key in weights:\n",
    "#     print(key, weights[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def uniform_element_selection(wt, s_shape):\n",
    "    assert wt.dim() == len(s_shape), \"Tensors have different number of dimensions\"\n",
    "    ws = wt.clone()\n",
    "    for dim in range(wt.dim()):\n",
    "        assert wt.shape[dim] >= s_shape[dim], \"Teacher's dimension should not be smaller than student's dimension\"  # determine whether teacher is larger than student on this dimension\n",
    "        if wt.shape[dim] % s_shape[dim] == 0:\n",
    "            step = wt.shape[dim] // s_shape[dim]\n",
    "            indices = torch.arange(s_shape[dim]) * step\n",
    "        else:\n",
    "            indices = torch.round(torch.linspace(0, wt.shape[dim]-1, s_shape[dim])).long()\n",
    "        \n",
    "        #print(indices)\n",
    "        ws = torch.index_select(ws, dim, indices)\n",
    "    assert ws.shape == s_shape\n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.756590592\n",
      "model.embed_tokens.weight torch.Size([156032, 4096]) torch.Size([156032, 2048])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.q_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.k_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.v_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([2048, 2048])\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.q_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.k_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.v_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([2048, 2048])\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.q_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.k_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.v_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([2048, 2048])\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.q_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.k_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.v_proj.bias torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([2048, 2048])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([2048, 2048])\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096]) torch.Size([2048])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096]) torch.Size([2048])\n",
      "model.norm.weight torch.Size([4096]) torch.Size([2048])\n",
      "lm_head.weight torch.Size([156032, 4096]) torch.Size([156032, 2048])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "#   \"hidden_size\": 4096,\n",
    "#   \"intermediate_size\": 11008,\n",
    "#   \"num_attention_heads\": 32,\n",
    "#   \"num_hidden_layers\": 32,\n",
    "#   \"num_key_value_heads\": 32,\n",
    "\n",
    "def make_small_qwen_audio(large_qwenaudio, hidden_size=2048, num_hidden_layers=4, intermediate_size=2048):\n",
    "\n",
    "    teacher_model = large_qwenaudio.language_model\n",
    "    student_config = copy.deepcopy(teacher_model.config)\n",
    "\n",
    "    student_config.hidden_size = hidden_size\n",
    "    student_config.num_hidden_layers = num_hidden_layers\n",
    "    student_config.intermediate_size = intermediate_size\n",
    "\n",
    "    student_model = AutoModelForCausalLM.from_config(student_config, attn_implementation=config._attn_implementation)\n",
    "\n",
    "    print(student_model.num_parameters() / 1e9)\n",
    "\n",
    "    teacher_weights = teacher_model.state_dict()\n",
    "    student_weights = student_model.state_dict()\n",
    "    weight_selection = {}\n",
    "    for key in student_weights.keys():\n",
    "        # We don't perform weight selection on classification head by default. Remove this constraint if target dataset is the same as teacher's.\n",
    "        # if \"head\" in key:\n",
    "        #     continue\n",
    "        print(key, teacher_weights[key].shape, student_weights[key].shape)\n",
    "        s_shape = student_weights[key].shape\n",
    "        weight_selection[key] = uniform_element_selection(teacher_weights[key], s_shape)\n",
    "        \n",
    "    student_weights.update(weight_selection)\n",
    "    student_model.load_state_dict(student_weights)\n",
    "\n",
    "    # 修改 multi_modal_projector 的 linear 层\n",
    "    large_qwenaudio.config.text_config.hidden_size = hidden_size\n",
    "    large_qwenaudio.config.text_config.num_hidden_layers = num_hidden_layers\n",
    "    large_qwenaudio.config.text_config.intermediate_size = intermediate_size\n",
    "    student_multi_modal_projector = Qwen2AudioMultiModalProjector(large_qwenaudio.config)\n",
    "    \n",
    "    large_qwenaudio.multi_modal_projector = student_multi_modal_projector\n",
    "    large_qwenaudio.language_model = student_model\n",
    "\n",
    "    return large_qwenaudio\n",
    "\n",
    "\n",
    "small_model = make_small_qwen_audio(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 9])\n",
      "attention_mask torch.Size([1, 9])\n",
      "input_features torch.Size([1, 128, 3000])\n",
      "feature_attention_mask torch.Size([1, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio features shape: torch.Size([1, 750, 2048])\n",
      "Input embeddings shape: torch.Size([1, 9, 2048])\n",
      "Input IDs shape: torch.Size([1, 9])\n",
      "Attention mask shape: torch.Size([1, 9])\n",
      "Position IDs shape: torch.Size([1, 9])\n",
      "Audio output lengths shape: torch.Size([1])\n",
      "num_audios:  1  max_audio_tokens:  750  embed_dim:  2048\n",
      "tensor([[ 86958, 151651,  23978,  58684, 101830,  87331,  85059, 151952, 101930,\n",
      "          95011,  39426,  70226, 100446, 100562, 154925, 132258,  83050,  87331,\n",
      "         138139,  48402,  42180,  57647, 134631,  95773,  94366, 109245, 101830,\n",
      "         132258,   1680, 151651, 120172,  47499,  49646, 154922, 132258,  37968,\n",
      "         144160,  87331,  61047,  87331,  38063, 121688, 112688,  39366, 102928,\n",
      "          60399, 135873,  89039,  42339,  77698,  99636, 119706,  89195, 151994,\n",
      "          80042,  89039,  92188,  47247,  90732,  80042,  98082,  68028, 141014,\n",
      "         121000, 136432,  36316,  55356,  54803, 110973,  99846,  88291, 121688,\n",
      "         114184,  50990,  22741,  24345, 103831,  44851,  95873, 103128,  81806,\n",
      "         154922,  52328,  89650, 103403,  55356,  78760,  98614,  87331,  97030,\n",
      "         151993, 109760,  70435,  88158,  55984,  51914,  85474,  72329, 107697,\n",
      "          62640, 100273,  68028, 111735, 116943,  42019,  77385,  85132,  69792,\n",
      "         100273,  71343,  39504, 115711,  38846,  12683, 103403,  55356, 127836,\n",
      "         151991,  68028,  36251, 137395, 102203,  72329, 115224,  18123, 152487,\n",
      "          53081,  55356, 127836,  81019,  74252, 110085, 129373,  63457,  63457,\n",
      "         100996, 103403,  62785, 122158,  24345, 121688,  29695,  54498,  81806,\n",
      "          63918,  77578,  99202, 103665, 153445, 131618,  89039, 113854, 103403,\n",
      "          24345,  77385,  61149, 118801, 134788,  99594, 131618,  76621, 144160,\n",
      "         151991,  36479, 103403, 112049,  24345, 103403,  24345,  41326, 142912,\n",
      "         129373,  59436,  80766,  68702,  98731, 103403,  24345, 100350, 142960,\n",
      "         153398,  89411, 120422,  22704,  24345, 105766,  42339, 105141,  70769,\n",
      "         113439,  42339,  86775, 103403, 103403, 129373,  76489,  99262,  42339,\n",
      "         102065, 103403, 103403,  92663,  93236, 119852,  42339,  52441,  42339,\n",
      "         103403,  42339, 103403, 103403,  38315,  53081, 103403,  24345,   1437,\n",
      "         143655, 103665,  54406, 103403,  24345, 103403, 129373,  53081, 103403,\n",
      "          24345, 151992,  42339,  70074, 103403, 103403,  82005,  65554,  82802,\n",
      "          63585, 113439,  42339,  42339, 103403,  24345, 110618,  92623, 107374,\n",
      "          37086, 154921, 103403, 103403]])\n",
      "______________________\n",
      "ModifiedDate eskSTE垒SingleOrDefault.dependencies长期:params口WXYZ牙婆wł�SingleOrDefaultており\tsetTimeoutsteadbeftürkconsts Ingram分成垒włamespace爰ASF_lazywłzierＸSingleOrDefault','.SingleOrDefault_mar谵眼看��态盲-dbПетер<>(\"platINCREMENT似帏TIMER.dropdown<>(\"_fatal-mfDSP.dropdownsar-initializedغض腆んですよertoolsrysler/Login看点顾lea谵诚意assesrez�憧�inke浑/navbar.ns<!--\n",
      "衍ryslerDRAMоличествоSingleOrDefaultGLenum哨/Internal.isSuccessInternalServerErrorptunecrastSeleccion女神|r徒-initialized建成后绚丽DTDtoLocale.za�徒.PreparedStatementERTICAL片面 Telerikidebar衍ryslerípio-initializedartisan peça吹Seleccion特派eland viewTyperyslerípioFIXME-scalable万家większ[Double[Double阐衍lei吖�谵 gehınd/navbarúmerostower�杞lâ<>(\"其中一个衍�toLocale周期详解تض趣lâInvariantＸ-License衍有期�衍�rysIÊNwiększ/Editcbcestructionenor衍�获得تسويق(){}\n",
      "\n",
      "笊期�有助plat特朗族葵platTextEdit衍衍większ_deps助plat线上衍衍ResponseStatus arsch郅plat\\Htmlplat衍plat衍衍PLEMENT viewType衍�ifyЛенин杞nez衍�衍większ viewType衍�plat声衍衍_signup.gcOFFSEThazi葵platplat衍�知情 McKenzie征收toUpperCase衍衍\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "small_model.eval()\n",
    "# model.language_model = language_model\n",
    "# test a bit\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n",
    "\n",
    "prompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\n",
    "audio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\n",
    "inputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n",
    "\n",
    "for k, v in inputs.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "generated_ids = small_model.generate(**inputs, max_length=256)\n",
    "generated_ids = generated_ids[:, inputs.input_ids.size(1):]\n",
    "print(generated_ids)\n",
    "response = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(\"______________________\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the small model\n",
    "save_name = \"qwen2_audio_0.7b\"\n",
    "save_path = f\"/home/AI_repo/sean/vt2/models/{save_name}\"\n",
    "small_model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "def compare_models(model1, model2):\n",
    "\n",
    "    print(\"model1 training: \", model1.training, \" model2 training: \", model2.training)\n",
    "\n",
    "    # 先比较config\n",
    "    # Compare the configurations\n",
    "    config1 = model1.config.to_dict()\n",
    "    config2 = model2.config.to_dict()\n",
    "\n",
    "    print(config1)\n",
    "    print(\"______________________\")\n",
    "    print(config2)\n",
    "\n",
    "    for key in config1:\n",
    "        if key in config2:\n",
    "            if config1[key] != config2[key]:\n",
    "                print(f\"Different in {key}: Model1 - {config1[key]}, Model2 - {config2[key]}\")\n",
    "        else:\n",
    "            print(f\"Key {key} not found in Model2's config\")\n",
    "\n",
    "    for key in config2:\n",
    "        if key not in config1:\n",
    "            print(f\"Key {key} not found in Model1's config\")\n",
    "   \n",
    "\n",
    "    # Retrieve the state dictionaries\n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "\n",
    "    # For each parameter in the state dictionaries\n",
    "    for param1, param2 in zip(state_dict1.items(), state_dict2.items()):\n",
    "        # Unpack the parameter names and tensors\n",
    "        name1, tensor1 = param1\n",
    "        name2, tensor2 = param2\n",
    "\n",
    "        # Check the names are the same (they should be, if the models are of the same architecture)\n",
    "        if name1 == name2:\n",
    "            try:\n",
    "                # Calculate the difference between the two tensors\n",
    "                difference = torch.nn.functional.mse_loss(tensor1, tensor2)\n",
    "                \n",
    "                print(f\"Difference in {name1}: {difference}\")\n",
    "            except:\n",
    "                print(f\"Cannot compare {name1} and {name2}\")\n",
    "        else:\n",
    "            print(f\"Parameter names do not match: {name1}, {name2}\")\n",
    "\n",
    "\n",
    "\n",
    "compare_models(language_model, small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwenaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
